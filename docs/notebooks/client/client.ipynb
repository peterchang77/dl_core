{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of Client\n",
    "\n",
    "The `Client()` object as part of the `dl_train.client`\n",
    "module provides a simple yet powerful interface for loading and preprocessing\n",
    "data in the context of neural network training. With a properly formatted\n",
    "`*.csv` file referencing data location, a `Client()` object can be initialized\n",
    "with nothing more than a simple `*.yml` configuration file. In this notebook we\n",
    "will explore the following functionality:\n",
    "\n",
    "**Configuration**\n",
    "\n",
    "* `*.csv` file\n",
    "format\n",
    "* `*.yml` file settings \n",
    "* rates\n",
    "* specs\n",
    "* split\n",
    "\n",
    "**Loading Data**\n",
    "\n",
    "* via `self.get(...)` method\n",
    "* via `self.generator(...)` method\n",
    "* via `self.create_generators(...)` method\n",
    "\n",
    "**Testing**\n",
    "\n",
    "* via\n",
    "`self.test(...)` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up\n",
    "\n",
    "Let us first set up the required imports and\n",
    "dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_train.client import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "A fully functional `Client()` object can be instantiated with a\n",
    "`*.yml` file containing the necessary configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Instantiate a new Client\n",
    "client = Client('./client.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the instantiated client, a single call to the `client.create_generators(...)` method will return Python generators for training and validation data that confrom to the Tensorflow/Keras API e.g. can be trained directly with `model.fit_generator(...)`. \n",
    "\n",
    "The following shows the contents of the `./client.yml` file in the current directory:\n",
    "\n",
    "```yml\n",
    "_db: ../../../../dl_utils/data/bet/ymls/db.yml\n",
    "rates:\n",
    "  sampling:\n",
    "    fg: 0.5\n",
    "    bg: 0.5 \n",
    "  training:\n",
    "    train: 0.8\n",
    "    valid: 0.2\n",
    "specs:\n",
    "  xs:\n",
    "    dat:\n",
    "      dtype: float32\n",
    "      loads: dat\n",
    "      norms:\n",
    "        clip: \n",
    "          min: 0\n",
    "          max: 256\n",
    "        shift: 64\n",
    "        scale: 64\n",
    "      shape:\n",
    "      - 1\n",
    "      - 512\n",
    "      - 512\n",
    "      - 1\n",
    "  ys:\n",
    "    bet:\n",
    "      dtype: uint8\n",
    "      loads: bet \n",
    "      norms: null\n",
    "      shape:\n",
    "      - 1\n",
    "      - 512\n",
    "      - 512\n",
    "      - 1\n",
    "split: \n",
    "  fold: -1\n",
    "  cohorts:\n",
    "    fg: fg\n",
    "    bg: bg \n",
    "```\n",
    "\n",
    "Let us walk through this configuration file in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database\n",
    "\n",
    "```yml\n",
    "_db: ../../../../dl_utils/data/bet/ymls/db.yml\n",
    "```\n",
    "\n",
    "The prepared database file path is set as `_db` in the first line of the configuration file; it may be referenced as either a `*.csv` for `*.yml` file. See tutorial on generic database objects for further additional information.\n",
    "\n",
    "For the `Client()` object, a special database is required whereby **each row** in the `*.csv` file represents a unique signature for every training **example** in the dataset. For 3D datasets, this may yield a new row for every slice or slab in the entire volume (if the network is designed to train on a slice- or slab- basis). \n",
    "\n",
    "This schematic shows a representative `*.csv` file:\n",
    "\n",
    "```\n",
    "             fname-dat       fname-lbl       coord    bg      fg        mu      sd\n",
    "             \n",
    "patient_00   /path/to/file   /path/to/file   0.0      True    False     100.0   20.0\n",
    "patient_00   /path/to/file   /path/to/file   0.1      True    False     100.0   20.0\n",
    "patient_00   /path/to/file   /path/to/file   0.2      True    False     100.0   20.0\n",
    "patient_00   /path/to/file   /path/to/file   0.3      False   True      100.0   20.0\n",
    "patient_00   /path/to/file   /path/to/file   0.4      False   True      100.0   20.0\n",
    "patient_00   /path/to/file   /path/to/file   0.5      False   True      100.0   20.0\n",
    "...\n",
    "patient_00   /path/to/file   /path/to/file   1.0      True    False     100.0   20.0\n",
    "patient_01   /path/to/file   /path/to/file   0.0      True    False     150.0   25.0\n",
    "...\n",
    "\n",
    "```\n",
    "\n",
    "This format is consistent with the standard database object format (e.g. all columns containing filenames must be prefixed with `fname-`, the row indices represent patient studyids, etc). In addition, the following three types of header columns should be considered as needed:\n",
    "\n",
    "1. If you are using a 3D individual slice- or slab-based network, you will need to provide a column name `coord` that contains the **normalized** coordinate between `[0, 1]` for that slice- or slab- of data. \n",
    "\n",
    "2. If you are using any stratified sampling technique, you will need to create columns containing a boolean vector the corresponds to each individual cohort. In the example above, we have two cohorts (`bg` and `fg` which correspond to slices containing background and foreground mask values, respectively) for which we plan to sample from at a balanced 50/50% distribution. Keep in mind that the defined cohorts **do not** need to be either mutually exclusive or inclusive of the entire dataset---they simply need to correspond to cohorts for which you plan to implement stratified sampling.  \n",
    "\n",
    "3. If you are using parameters for image preprocessing that cannot be dynamically inferred during data loading (e.g. the mean of the entire 3D volume when loading data in a slice-by-slice manner), you will need to create column(s) that contain the required information. Commonly used metrics include the mean and standard deviation of an entire 3D volume used to perform z-score normalization for MRI exams. See `specs` below for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Specifications\n",
    "\n",
    "```yml\n",
    "specs:\n",
    "  batch: 16\n",
    "  xs:\n",
    "    dat:\n",
    "      dtype: float32\n",
    "      loads: dat\n",
    "      norms:\n",
    "        clip: \n",
    "          min: 0\n",
    "          max: 256\n",
    "        shift: 64\n",
    "        scale: 64\n",
    "      shape:\n",
    "      - 1\n",
    "      - 512\n",
    "      - 512\n",
    "      - 1\n",
    "  ...\n",
    "```\n",
    "\n",
    "The `specs` entry in the configuration file defines the type, shape and origin of data to be loaded, as well as any necessary normalization operations to perform on the loaded data. The two primary entries in `specs` are `xs` and `ys` which represent training input data and output label(s), respectively. For each entry `xs` and `ys`, one or many individual volumes may be defined depending on architecture requirements.\n",
    "\n",
    "For each individual volume, the following parameters must be defined:\n",
    "\n",
    "* `dtype`: str representing the data type (often `float32` for input data, `uint8` for output label)\n",
    "* `shape`: 4D shape of input (Z x Y x X x channel); note channel often == 1; for 2D data, Z == 1\n",
    "* `loads`: column (if any) to load data from (see below for more details)\n",
    "* `norms`: normalization parameter (see below for more details); use `null` keyword to indicate no normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading\n",
    "\n",
    "The `loads` entry determines the column name from which to populate data for this input or output variable. If the column is part of the `fnames` DataFrame then the corresponding file will be loaded (at the slice location specified by `coord` if the data is 3D or 4D). If the column is part of the `header` DataFrame then the raw value will be coverted to a Numpy array.\n",
    "\n",
    "If no corresponding data should be loaded, use the keyword `null`. In this scenario, the corresponding array should be dynamically defined in an overloaded `client.preprocess(...)` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization parameters\n",
    "\n",
    "A total of up to three parameters can be used to define a data normalization strategy:\n",
    "\n",
    "* clip (includes min and/or max)\n",
    "* shift\n",
    "* scale\n",
    "\n",
    "These parameters are implemented in the following normalization formula:\n",
    "\n",
    "```\n",
    "arr = (arr.clip(min=..., max=...) - shift) / scale\n",
    "```\n",
    "\n",
    "There are three ways to define these normalization parameters:\n",
    "\n",
    "1. **Constant value (integer or float)**. Use a literal value if you know that the required parameter value is constant for all data. This is most commonly used for CT imaging data (voxel values are precalibrated as Hounsfeld Units).\n",
    "\n",
    "```yml\n",
    "norms:\n",
    "  clip: \n",
    "    min: 0\n",
    "    max: 256\n",
    "  shift: 64\n",
    "  scale: 64\n",
    "```\n",
    "\n",
    "2. **Numpy function (@ keyword)**. Use a keyword prefixed by `@` to represent valid Numpy function (e.g. `np.(...)`) to be applied dynamically to each input image upon load. Common usage here includes the `@mean` and `@std` functions to implement a simple z-score transformation. *Important*: these methods should be used only if the input image is guaranteed to provide valid return(s); as a counterexample, the standard deviation on a uniform 2D image (seen at the top and bottom of 3D volumes) will be undefined. Thus this should rarely be used for 3D or 4D volumes (see option 3 below instead).\n",
    "\n",
    "```yml\n",
    "norms: \n",
    "  shift: @mean\n",
    "  scale: @std\n",
    "```\n",
    "\n",
    "3. **Column name**. Use a regular str (no @ prefix) to indicate a column name containing any custom normalization parameters. For 3D volumes, a common strategy is to normalize each slice by the mean and standard deviation of the entire volume. However, since loading the entire 3D volume for each slice during training is inefficient, volume statistics can instead be stored in each row of the `*.csv` file and simply referenced for preprocessing. A z-score normalization implemented using this technique is the recommended approach for MR imaging data.\n",
    "\n",
    "```yml\n",
    "norms:\n",
    "  shift: mu\n",
    "  scale: sd\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other `specs` parameters\n",
    "\n",
    "* **`batch`**: (optional) training batch size\n",
    "* **`tiles`**: (optional) a 4-element boolean list corresponding to axes that may be expanded during inference\n",
    "* **`infos`**: (optional) custom `infos` dictionary values to pass to data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tear Down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
