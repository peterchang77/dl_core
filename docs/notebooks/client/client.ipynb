{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of Client\n",
    "\n",
    "The `Client()` object as part of the `dl_train.client` module provides a simple yet powerful interface for loading and preprocessing data in the context of neural network training. With a properly formatted `*.csv` file referencing data location, a `Client()` object can be initialized with nothing more than a simple `*.yml` configuration file. In this notebook we will explore the following functionality:\n",
    "\n",
    "**Configuration**\n",
    "\n",
    "* `*.csv` fileformat\n",
    "* `*.yml` file settings \n",
    "* batch\n",
    "* specs\n",
    "\n",
    "**Loading Data**\n",
    "\n",
    "* via `self.get(...)` method\n",
    "* via `self.generator(...)` method\n",
    "* via `self.create_generators(...)` method\n",
    "\n",
    "**Testing**\n",
    "\n",
    "* via\n",
    "`self.test(...)` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up\n",
    "\n",
    "Let us first set up the required imports and\n",
    "dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_train.client import Client\n",
    "from dl_utils.general import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "A fully functional `Client()` object can be instantiated with a `*.yml` file containing the necessary configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Instantiate a new Client\n",
    "client = Client('./client.yml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the instantiated client, a single call to the `client.create_generators(...)` method will return Python generators for training and validation data that confrom to the Tensorflow/Keras API e.g. can be trained directly with `model.fit_generator(...)`. \n",
    "\n",
    "The following shows the contents of the `./client.yml` file in the current directory:\n",
    "\n",
    "```yml\n",
    "_db: ../../../../dl_utils/data/bet/ymls/db.yml\n",
    "batch:\n",
    "  fold: -1\n",
    "  sampling:\n",
    "    fg: 0.5\n",
    "    bg: 0.5 \n",
    "specs:\n",
    "  xs:\n",
    "    dat:\n",
    "      dtype: float32\n",
    "      loads: dat\n",
    "      norms:\n",
    "        clip: \n",
    "          min: 0\n",
    "          max: 256\n",
    "        shift: 64\n",
    "        scale: 64\n",
    "      shape:\n",
    "      - 1\n",
    "      - 512\n",
    "      - 512\n",
    "      - 1\n",
    "  ys:\n",
    "    bet:\n",
    "      dtype: uint8\n",
    "      loads: bet \n",
    "      norms: null\n",
    "      shape:\n",
    "      - 1\n",
    "      - 512\n",
    "      - 512\n",
    "      - 1\n",
    "```\n",
    "\n",
    "Let us walk through this configuration file in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database\n",
    "\n",
    "```yml\n",
    "_db: ../../../../dl_utils/data/bet/ymls/db.yml\n",
    "```\n",
    "\n",
    "The prepared database file path is set as `_db` in the first line of the configuration file; it may be referenced as either a `*.csv` for `*.yml` file. See tutorial on generic database objects for further additional information.\n",
    "\n",
    "For the `Client()` object, a special database is required whereby **each row** in the `*.csv` file represents a unique signature for every training **example** in the dataset. For 3D datasets, this may yield a new row for every slice or slab in the entire volume (if the network is designed to train on a slice- or slab- basis). \n",
    "\n",
    "This schematic shows a representative `*.csv` file:\n",
    "\n",
    "```\n",
    "             fname-dat       fname-lbl       coord    bg      fg        mu      sd      valid\n",
    "             \n",
    "patient_00   /path/to/file   /path/to/file   0.0      True    False     100.0   20.0    3\n",
    "patient_00   /path/to/file   /path/to/file   0.1      True    False     100.0   20.0    3\n",
    "patient_00   /path/to/file   /path/to/file   0.2      True    False     100.0   20.0    3\n",
    "patient_00   /path/to/file   /path/to/file   0.3      False   True      100.0   20.0    3\n",
    "patient_00   /path/to/file   /path/to/file   0.4      False   True      100.0   20.0    3\n",
    "patient_00   /path/to/file   /path/to/file   0.5      False   True      100.0   20.0    3\n",
    "...\n",
    "patient_00   /path/to/file   /path/to/file   1.0      True    False     100.0   20.0    3\n",
    "patient_01   /path/to/file   /path/to/file   0.0      True    False     150.0   25.0    0\n",
    "...\n",
    "\n",
    "```\n",
    "\n",
    "This format is consistent with the standard database object format (e.g. all columns containing filenames must be prefixed with `fname-`, the row indices represent patient studyids, etc). In addition, the following four types of header columns should be considered as needed:\n",
    "\n",
    "1. *(required)* The `valid` column contains a random integer between `[0, n)` where `n` represents the total number of cross-validation folds for the specific experiment. For a standard 5-fold cross-validation, the integers range from `[0, 4]` . Note that **all rows for the patient must be in the same cross-validation fold**.  \n",
    "\n",
    "2. If you are using a 3D individual slice- or slab-based network, you will need to provide a column name `coord` that contains the **normalized** coordinate between `[0, 1]` for that slice- or slab- of data. \n",
    "\n",
    "3. If you are using any stratified sampling technique, you will need to create columns containing a boolean vector the corresponds to each individual cohort. In the example above, we have two cohorts (`bg` and `fg` which correspond to slices containing background and foreground mask values, respectively) for which we plan to sample from at a balanced 50/50% distribution. Keep in mind that the defined cohorts **do not** need to be either mutually exclusive or inclusive of the entire dataset---they simply need to correspond to cohorts for which you plan to implement stratified sampling.  \n",
    "\n",
    "4. If you are using parameters for image preprocessing that cannot be dynamically inferred during data loading (e.g. the mean of the entire 3D volume when loading data in a slice-by-slice manner), you will need to create column(s) that contain the required information. Commonly used metrics include the mean and standard deviation of an entire 3D volume used to perform z-score normalization for MRI exams. See `specs` below for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Composition\n",
    "\n",
    "```yml\n",
    "batch:\n",
    "  fold: -1\n",
    "  sampling:\n",
    "    fg: 0.5\n",
    "    bg: 0.5 \n",
    "```\n",
    "\n",
    "The composition (source) of each data batch used for network training derives from two primary factors:\n",
    "\n",
    "* cross validation fold for current experiment\n",
    "* stratified sampling (if any)\n",
    "\n",
    "### Validation fold\n",
    "\n",
    "For a given experiment, the specified `fold` represents the data split to use for validation. For example if fold is set to 0, then all rows with a value of 0 in the `valid` column will be used for validation. Set the fold to -1 in order to use **all data** for both training and validation (e.g. no validation).\n",
    "\n",
    "### Stratified sampling\n",
    "\n",
    "For certain experiments, it is critical for specific rows to be used during training at a higher or lower frequency than others. This technique is known as stratified sampling. A common example is to oversample from the images containing a positive disease finding (e.g. pathology tends to be rare) so that the distribution of positive and negative training examples is balanced.\n",
    "\n",
    "The syntax to define stratified sampling is a series of key-value pairs such that:\n",
    "\n",
    "* key: name of header column, containing a boolean vector, that defines rows which are part of a specific cohort\n",
    "* value: rate between `[0, 1]` for which to sample from this specific cohort\n",
    "\n",
    "All values in total must add to 1.0.\n",
    "\n",
    "In the example above, the `fg` column contains a value of True or False for each row depending on whether or not a positive mask value is present in the corresponding label. The `bg` column contains the opposite information. \n",
    "\n",
    "### Optional `batch` parameters\n",
    "\n",
    "* `size`: batch size; if not specified here, pass the batch size as an argument into `client.create_generators(...)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Specifications\n",
    "\n",
    "```yml\n",
    "specs:\n",
    "  xs:\n",
    "    dat:\n",
    "      dtype: float32\n",
    "      loads: dat\n",
    "      norms:\n",
    "        clip: \n",
    "          min: 0\n",
    "          max: 256\n",
    "        shift: 64\n",
    "        scale: 64\n",
    "      shape:\n",
    "      - 1\n",
    "      - 512\n",
    "      - 512\n",
    "      - 1\n",
    "  ...\n",
    "```\n",
    "\n",
    "The `specs` entry in the configuration file defines the type, shape and origin of data to be loaded, as well as any necessary normalization operations to perform on the loaded data. The two primary entries in `specs` are `xs` and `ys` which represent training input data and output label(s), respectively. For each entry `xs` and `ys`, one or many individual volumes may be defined depending on architecture requirements.\n",
    "\n",
    "For each individual volume, the following parameters must be defined:\n",
    "\n",
    "* `dtype`: str representing the data type (often `float32` for input data, `uint8` for output label)\n",
    "* `shape`: 4D shape of input (Z x Y x X x channel); note channel often == 1; for 2D data, Z == 1\n",
    "* `loads`: column (if any) to load data from (see below for more details)\n",
    "* `norms`: normalization parameter (see below for more details); use `null` keyword to indicate no normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading\n",
    "\n",
    "The `loads` entry determines the column name from which to populate data for this input or output variable. If the column is part of the `fnames` DataFrame then the corresponding file will be loaded (at the slice location specified by `coord` if the data is 3D or 4D). If the column is part of the `header` DataFrame then the raw value will be coverted to a Numpy array.\n",
    "\n",
    "If no corresponding data should be loaded, use the keyword `null`. In this scenario, the corresponding array should be dynamically defined in an overloaded `client.preprocess(...)` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization parameters\n",
    "\n",
    "A total of up to three parameters can be used to define a data normalization strategy:\n",
    "\n",
    "* clip (includes min and/or max)\n",
    "* shift\n",
    "* scale\n",
    "\n",
    "These parameters are implemented in the following normalization formula:\n",
    "\n",
    "```\n",
    "arr = (arr.clip(min=..., max=...) - shift) / scale\n",
    "```\n",
    "\n",
    "There are three ways to define these normalization parameters:\n",
    "\n",
    "1. **Constant value (integer or float)**. Use a literal value if you know that the required parameter value is constant for all data. This is most commonly used for CT imaging data (voxel values are precalibrated as Hounsfeld Units).\n",
    "\n",
    "```yml\n",
    "norms:\n",
    "  clip: \n",
    "    min: 0\n",
    "    max: 256\n",
    "  shift: 64\n",
    "  scale: 64\n",
    "```\n",
    "\n",
    "2. **Numpy function (@ keyword)**. Use a keyword prefixed by `@` to represent valid Numpy function (e.g. `np.(...)`) to be applied dynamically to each input image upon load. Common usage here includes the `@mean` and `@std` functions to implement a simple z-score transformation. *Important*: these methods should be used only if the input image is guaranteed to provide valid return(s); as a counterexample, the standard deviation on a uniform 2D image (seen at the top and bottom of 3D volumes) will be undefined. Thus this should rarely be used for 3D or 4D volumes (see option 3 below instead).\n",
    "\n",
    "```yml\n",
    "norms: \n",
    "  shift: @mean\n",
    "  scale: @std\n",
    "```\n",
    "\n",
    "3. **Column name**. Use a regular str (no @ prefix) to indicate a column name containing any custom normalization parameters. For 3D volumes, a common strategy is to normalize each slice by the mean and standard deviation of the entire volume. However, since loading the entire 3D volume for each slice during training is inefficient, volume statistics can instead be stored in each row of the `*.csv` file and simply referenced for preprocessing. A z-score normalization implemented using this technique is the recommended approach for MR imaging data.\n",
    "\n",
    "```yml\n",
    "norms:\n",
    "  shift: mu\n",
    "  scale: sd\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional `specs` parameters\n",
    "\n",
    "* `tiles`: a 4-element boolean list corresponding to axes that may be expanded during inference\n",
    "* `infos`: custom `infos` dictionary values to pass to data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "\n",
    "After creating a `Client()` object, data may be retrieved using one of the following three methods:\n",
    "\n",
    "* via `self.get(...)` method\n",
    "* via `self.generator(...)` method\n",
    "* via `self.create_generators(...)` method\n",
    "\n",
    "Note, for the purposes of algorithm training, the `self.create_generators(...)` method will be most commonly utilized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Load - `self.get(...)`\n",
    "\n",
    "The `self.get(...)` method can be used to manually load a single row of data. The return is a single Python dictionary with a structure that matches the `specs` attribute above:\n",
    "\n",
    "```python\n",
    "arrays = {\n",
    "    'xs': {'dat': ...},\n",
    "    'ys': {'bet': ...}}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load a single train or valid case\n",
    "arrays = client.get(split='train')\n",
    "arrays = client.get(split='valid')\n",
    "\n",
    "# --- Load a single fg or bg case\n",
    "arrays = client.get(cohort='fg')\n",
    "arrays = client.get(cohort='bg')\n",
    "\n",
    "# --- Load a single case at row\n",
    "arrays = client.get(row=0)\n",
    "\n",
    "# --- Load a single random case\n",
    "arrays = client.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Load - `self.generator()`\n",
    "\n",
    "The `self.generator(...)` method can be used to create a Python generator that yields a batch of data from the specified split. If batch size was not previously defined in `specs`, it must be passed as an argument here. The yield of the generator is a tuple `(xs, ys)` that represents the stacked `xs` and `ys` dictionary of arrays above. This output conforms to the Tensorflow / Keras `model.fit_generator(...)` API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create train / valid generators\n",
    "gen_train = client.generator(split='train', batch_size=16)\n",
    "gen_valid = client.generator(split='valid', batch_size=16)\n",
    "\n",
    "# --- Iterate\n",
    "N = 10\n",
    "for i in range(N):\n",
    "    printp('Loading batch', (i + 1) / N)\n",
    "    train_batch = next(gen_train)\n",
    "    valid_batch = next(gen_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a convenience method, the `client.create_generators(...)` method combines the creation of train and valid generators into a single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create train / valid generators\n",
    "gen_train, gen_valid = client.create_generators(batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "The final section here describes some useful tips for testing and debugging the `Client()` object.\n",
    "\n",
    "## Checklist\n",
    "\n",
    "Answer **each question** in this checklist explicitly to ensure that the `client.yml` file is created properly:\n",
    "\n",
    "`_db`\n",
    "\n",
    "What is the full path to your database file (`*.yml` or `*.csv`)?\n",
    "\n",
    "`batch`\n",
    "\n",
    "What is your current experiment validation fold?\n",
    "\n",
    "What are the different cohorts (if any) that you plan to stratify sampling from?\n",
    "\n",
    "* Is each cohort defined by a corresponding header column?\n",
    "* Is each cohort column composed entirely of boolean (True/False) values?\n",
    "\n",
    "Do your sampling rates add to 1.0?\n",
    "\n",
    "`specs`\n",
    "\n",
    "What are you data input(s) (including masks)?\n",
    "\n",
    "What are you data output(s)?\n",
    "\n",
    "What is the data type (most commonly `float32` for `xs` and `uint8` for `ys`)?\n",
    "\n",
    "Which column does the data derive from?\n",
    "\n",
    "* Is the column in the `fnames` DataFrame or `header` DataFrame?\n",
    "* If the column is in `fnames`, does it exist?\n",
    "* If the column is in `header`, is it a numeric value?\n",
    "\n",
    "What is the data shape?\n",
    "\n",
    "* Is the shape defined by a full four-element list?\n",
    "* Is the data 2D (z == 1) or 3D (z > 1)?\n",
    "* Is the data 4D (channels > 1)?\n",
    "* Are you certain that the serialized data matches this shape?\n",
    "\n",
    "What are the normalization parameters?\n",
    "\n",
    "* For XR (or other 2D), consider:\n",
    "\n",
    "```yml\n",
    "norms:\n",
    "  shift: @mean\n",
    "  scale: @std\n",
    "  \n",
    "```\n",
    "\n",
    "* For CT, consider:\n",
    "\n",
    "```yml\n",
    "norms:\n",
    "  clip:\n",
    "    min: 0\n",
    "    max: 256\n",
    "  shift: 64\n",
    "  scale: 64\n",
    "  \n",
    "```\n",
    "\n",
    "* For MR, consider manual custom columns for mean and SD:\n",
    "\n",
    "```yml\n",
    "norms:\n",
    "  shift: mu\n",
    "  scale: sd\n",
    "  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tear Down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
