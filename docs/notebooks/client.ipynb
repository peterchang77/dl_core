{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of Client\n",
    "\n",
    "The pipeline for loading data for machine learning training is a critical process that requires thoughtful and deliberate planning. Key considerations that need to be accounted for include:\n",
    "\n",
    "* keeping track of all data(s) and/or label(s) for each individual training case\n",
    "* consistent stratified sampling between training and validation splits\n",
    "* consistent stratified sampling between different data cohorts (e.g. from different hospitals or sources)\n",
    "* randomization of data loading order between epochs\n",
    "* any \"real time\" data preprocessing (e.g. normalization)\n",
    "\n",
    "Other advanced functionality may include:\n",
    "\n",
    "* in-memory loading of all data before training starts (if dataset is small)\n",
    "* asynchronous loading of data (if dataset is large)\n",
    "\n",
    "In this tutorial, we will cover the basics of creating a `client` for loading data, covering much of the key functionality described above. For each individual project, you must create your **own individualized** client for loading data. To help you get started, a template fully functional `client` is availabe in this repository at `/dl_core/clients/client.py`. Example usage of this module is available at the end of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts\n",
    "\n",
    "All terms in **bold** will reference specific concepts that are reused throughout this tutorial. Please review these terms before proceeding further.\n",
    "\n",
    "Machine learning algorithms require data to be **split** into *training* and *validation* sets. All training paradigms *require* this baseline division of data. For the purposes of this tutorial, a **split** represents the current usage of any particular example as *training* or *validation* data. Note that in the setting of cross-validation, all data will be used as *both* training and validation cases at different points during algorithm development.\n",
    "\n",
    "For certain problems, it is necessary to further subdivide data into **cohorts**. For the purposes of this tutorial, a **cohort** represents *any arbitrary* division of data into user defined subgroups. Why are **cohorts** important? It turns out that stratified sampling (e.g. selecting data at fixed rates from specific cohorts) oftentimes improves training dynamics for heterogenous datasets, including those commonly seen in medical problems. For example, given the low prevalence of most disease states, it is often beneficial to load data at an even 50-50% distribution between positive and negative examples (e.g supersample from the positive category). \n",
    "\n",
    "Given the above, two different sampling **rates** are defined:\n",
    "\n",
    "* **training_rate**: represents the rate of randomly selecting training / validation cases\n",
    "* **sampling_rate**: represents the rate at which each individual **cohort** is sampled from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Data Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the need to careful subselect splits or cohorts of data, it is often valuable to first extract key characteristics of data *first* as an independent step prior to loading any data. In this tutorial, implementation will including the following steps:\n",
    "\n",
    "* find all data(s) and/or label(s) for each individual training case\n",
    "* extract summary information about each training case\n",
    "* aggregate all summarized data\n",
    "* determine training / validation splits\n",
    "* serialize summary as a Pickle file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Data\n",
    "\n",
    "This portion of the code will be most the variable depending on the directory hierachy for the data in your project. In general, the goal will be to create a list of dictionaries, with each key-value pair representing a single full file path name. For example:\n",
    "\n",
    "```\n",
    "d = {\n",
    "    'dat_0': /full/path/to/dat/0,\n",
    "    'lbl_0': /full/path/to/lbl/0,\n",
    "    'lbl_1': /full/path/to/lbl/1, ... }\n",
    "```\n",
    "\n",
    "Note that the keys you choose can be arbitrary, as long as you remember what is what!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example, the data is currently organized as follows:\n",
    "\n",
    "```\n",
    "/hdfs/[ID-...0]/dat.hdf5\n",
    "/hdfs/[ID-...0]/bet.hdf5\n",
    "/hdfs/[ID-...1]/dat.hdf5\n",
    "/hdfs/[ID-...1]/lbl.hdf5\n",
    "...\n",
    "```\n",
    "\n",
    "Assuming that your data is organized in a similar way, here is a simple method to generate such a list of dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "\n",
    "def find_data(query):\n",
    "    \"\"\"\n",
    "    Method to read all data and make summary dictionary \n",
    "\n",
    "    :params\n",
    "\n",
    "      (dict) query : {\n",
    "\n",
    "        'root': '/path/to/root/dir',\n",
    "        [key_00]: [query_00],\n",
    "        [key_01]: [query_01], ...\n",
    "\n",
    "      }\n",
    "      \n",
    "    \"\"\"\n",
    "    assert 'root' in query\n",
    "    assert len(query) > 1\n",
    "\n",
    "    root = query.pop('root')\n",
    "    keys = sorted(query.keys())\n",
    "\n",
    "    q = query.pop(keys[0])\n",
    "    matches = glob.glob('%s/**/%s' % (root, q), recursive=True)\n",
    "\n",
    "    DATA = []\n",
    "    \n",
    "    for n, m in enumerate(matches):\n",
    "\n",
    "        print('CREATING SUMMARY (%07i/%07i): %s' % (n + 1, len(matches), m))\n",
    "\n",
    "        d = {keys[0]: m}\n",
    "        b = os.path.dirname(m)\n",
    "\n",
    "        # --- Find other matches\n",
    "        for key in keys[1:]:\n",
    "            ms = glob.glob('%s/%s' % (b, query[key]))\n",
    "\n",
    "            if len(ms) == 1: \n",
    "                d[key] = ms[0]\n",
    "                \n",
    "        if len(d) == len(keys):\n",
    "            DATA.append(d)\n",
    "    \n",
    "    return DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code demonstrates example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set query\n",
    "query = {\n",
    "    'root': '../../data/hdfs',\n",
    "    'dat': 'dat.hdf5',\n",
    "    'bet': 'bet.hdf5'}\n",
    "\n",
    "# --- Find data\n",
    "data = find_data(query)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Slice Location\n",
    "\n",
    "Next, we need to identify information about **each slice** of data which will be used for algorithm training. To do so, we will first create a system to reference each individual slice data using an **index** and a **coord** variable:\n",
    "\n",
    "* **coord**: a *normalized* coordinate `[0, 1]` that represents the z-position of the slice\n",
    "* **index**: a value from `[0, n - 1]` representing the n-th sample in the dataset \n",
    "\n",
    "After all the data has been loaded and summarized, we should have two vectors, `coord` and `index`, *equal in size* to the total number of slices of all data. For example, if we had five volumes, each with 10 slices, then:\n",
    "\n",
    "```\n",
    "index = [0, 0, 0 ..., 1, 1, 1 ..., 9, 9, 9, ... 9, 9, 9]\n",
    "coord = [0, 1, 2 ..., 0, 1, 2 ..., 0, 1, 2, ... 7, 8, 9]\n",
    "```\n",
    "\n",
    "Assuming `data` contains a 4D Numpy volume, the following snippet pseudocode will accomplish this:\n",
    "\n",
    "```\n",
    "index = []\n",
    "coord = []\n",
    "\n",
    "for INDEX in range(len(EXAMS)):\n",
    "\n",
    "    [... load data ...]\n",
    "    \n",
    "    index.append(np.ones(data.shape[0], dtype='int') * INDEX)\n",
    "    coord.append(np.arange(data.shape[0]) / (data.shape[0] - 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See below for a rough implementation of the above concepts. Keep in mind we need an additional variable, `LABELED`, which references the *key* in query from which to load data and use for calculations (in our case, `bet`). To load `*.hdf5` files we will use the `dl_core.io.hdf5` library. See dedicated notebook for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "PATH = '../../'\n",
    "if PATH not in sys.path:\n",
    "    sys.path.append('../../')\n",
    "from dl_core.io import hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_summary(query, LABELED):\n",
    "    \"\"\"\n",
    "    Method to read all data and make summary dictionary \n",
    "\n",
    "    :params\n",
    "\n",
    "      (dict) query : {\n",
    "\n",
    "        'root': '/path/to/root/dir',\n",
    "        [key_00]: [query_00],\n",
    "        [key_01]: [query_01], ...\n",
    "\n",
    "      }\n",
    "\n",
    "    \"\"\"\n",
    "    assert 'root' in query\n",
    "    assert len(query) > 1\n",
    "    assert LABELED in query\n",
    "\n",
    "    root = query.pop('root')\n",
    "    keys = sorted(query.keys())\n",
    "\n",
    "    q = query.pop(keys[0])\n",
    "    matches = glob.glob('%s/**/%s' % (root, q), recursive=True)\n",
    "\n",
    "    DATA = []\n",
    "    META = {}\n",
    "    META['index'] = []\n",
    "    META['coord'] = []\n",
    "\n",
    "    for n, m in enumerate(matches):\n",
    "\n",
    "        d = {keys[0]: m}\n",
    "        b = os.path.dirname(m)\n",
    "\n",
    "        # --- Find other matches\n",
    "        for key in keys[1:]:\n",
    "            ms = glob.glob('%s/%s' % (b, query[key]))\n",
    "\n",
    "            if len(ms) == 1: \n",
    "                d[key] = ms[0]\n",
    "\n",
    "        # --- Caculate summary meta information from LABELED\n",
    "        if len(d) == len(keys):\n",
    "\n",
    "            data, _ = hdf5.load(d[LABELED])\n",
    "\n",
    "            # --- Aggregate information\n",
    "            META['index'].append(np.ones(data.shape[0], dtype='int') * len(DATA))\n",
    "            META['coord'].append(np.arange(data.shape[0]) / (data.shape[0] - 1))\n",
    "            DATA.append(d)\n",
    "\n",
    "    # --- Concatenate all vectors\n",
    "    META = {k: np.concatenate(v) for k, v in META.items()}\n",
    "    \n",
    "    return DATA, META"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code demonstrates example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set query\n",
    "query = {\n",
    "    'root': '../../data/hdfs',\n",
    "    'dat': 'dat.hdf5',\n",
    "    'bet': 'bet.hdf5'}\n",
    "\n",
    "# --- Find data\n",
    "data, meta = make_summary(query, LABELED='bet')\n",
    "print(data)\n",
    "print(meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Slice Data\n",
    "\n",
    "Finally, in addition to `index` and `coord`, we will want to read the provided label and extract some information regarding what values are present. This is important if later, we want to select slices that have a certain abnormality of finding present. In our example, the labels contain brain masks (1 == background, 2 == brain). Thus, we will now create two additional vectors, equal in size to the total number slices of all data (same as `index` and `coord`), that contain a binary True or False as to whether that slice contains background and/or brain.\n",
    "\n",
    "The following snippet of pseudocode will accomplish this for an arbitrary number of `CLASSES`:\n",
    "\n",
    "```\n",
    "META = {}\n",
    "for INDEX in range(len(EXAMS)):\n",
    "\n",
    "    [... load data ...]\n",
    "    \n",
    "    for c in range(CLASSES + 1):\n",
    "        s = np.sum(data == c, axis=(1, 2, 3)) > 0\n",
    "        META[c].append(s)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See below for a rough implementation of the above concepts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "PATH = '../../'\n",
    "if PATH not in sys.path:\n",
    "    sys.path.append('../../')\n",
    "from dl_core.io import hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_summary(query, LABELED, CLASSES=2):\n",
    "    \"\"\"\n",
    "    Method to read all data and make summary dictionary \n",
    "\n",
    "    :params\n",
    "\n",
    "      (dict) query : {\n",
    "\n",
    "        'root': '/path/to/root/dir',\n",
    "        [key_00]: [query_00],\n",
    "        [key_01]: [query_01], ...\n",
    "\n",
    "      }\n",
    "\n",
    "    \"\"\"\n",
    "    assert 'root' in query\n",
    "    assert len(query) > 1\n",
    "    assert LABELED in query\n",
    "\n",
    "    root = query.pop('root')\n",
    "    keys = sorted(query.keys())\n",
    "\n",
    "    q = query.pop(keys[0])\n",
    "    matches = glob.glob('%s/**/%s' % (root, q), recursive=True)\n",
    "\n",
    "    DATA = []\n",
    "    META = {c: [] for c in range(CLASSES + 1)}\n",
    "    META['index'] = []\n",
    "    META['coord'] = []\n",
    "\n",
    "    for n, m in enumerate(matches):\n",
    "\n",
    "        d = {keys[0]: m}\n",
    "        b = os.path.dirname(m)\n",
    "\n",
    "        # --- Find other matches\n",
    "        for key in keys[1:]:\n",
    "            ms = glob.glob('%s/%s' % (b, query[key]))\n",
    "\n",
    "            if len(ms) == 1: \n",
    "                d[key] = ms[0]\n",
    "\n",
    "        # --- Caculate summary meta information from LABELED\n",
    "        if len(d) == len(keys):\n",
    "\n",
    "            # --- Aggregate slice-by-slice label information\n",
    "            data, _ = hdf5.load(d[LABELED])\n",
    "\n",
    "            for c in range(CLASSES + 1):\n",
    "                s = np.sum(data == c, axis=(1, 2, 3)) > 0\n",
    "                META[c].append(s)\n",
    "\n",
    "            # --- Aggregate information\n",
    "            META['index'].append(np.ones(data.shape[0], dtype='int') * len(DATA))\n",
    "            META['coord'].append(np.arange(data.shape[0]) / (data.shape[0] - 1))\n",
    "            DATA.append(d)\n",
    "\n",
    "    # --- Concatenate all vectors\n",
    "    META = {k: np.concatenate(v) for k, v in META.items()}\n",
    "    \n",
    "    return DATA, META"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code demonstrates example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set query\n",
    "query = {\n",
    "    'root': '../../data/hdfs',\n",
    "    'dat': 'dat.hdf5',\n",
    "    'bet': 'bet.hdf5'}\n",
    "\n",
    "# --- Find data\n",
    "data, meta = make_summary(query, LABELED='bet', CLASSES=2)\n",
    "print(data)\n",
    "print(meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage\n",
    "\n",
    "In this section, we will explore example usage of the template `client` provided in this repoository. \n",
    "\n",
    "**IMPORTANT**: This `client` has been written to load 1 x 512 x 512 (e.g. single slice) arrays from the provided example head CT. You *will need to modify* this code for your own individual projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dl_core\n",
    "\n",
    "To use the `dl_core` library, you need to ensure that the repository path has been set. If you are using the python interpreter directlying (e.g. command line) you will need to add the repository path to the `$PYTHONPATH` environment variable. If you are using an iPython interface (e.g. including Jupyter) you will need to set the path using the `sys` module: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set PATH to dl_core library path\n",
    "PATH = '../../' \n",
    "\n",
    "# --- Use sys module to set $PYTHONPATH\n",
    "import sys\n",
    "if PATH not in sys.path:\n",
    "    sys.path.append(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_core.clients import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set default path locations\n",
    "SUMMARY_PATH = '../../data/pkls/summary.pkl'\n",
    "HDFS_PATH = '../../data/hdfs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a summary of the data\n",
    "\n",
    "Recall that in order to properly handle stratified sampling requirements, we need to know more information about the underlying data (e.g. which slice(s) are positive, etc). This information will then be used to randomize and organize cohorts for future data loading pipelines. See section above for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(SUMMARY_PATH=SUMMARY_PATH)\n",
    "\n",
    "client.make_summary(\n",
    "    query={\n",
    "        'root': HDFS_PATH,\n",
    "        'dat': 'dat.hdf5',\n",
    "        'bet': 'bet.hdf5'},\n",
    "    LABELED='bet',\n",
    "    CLASSES=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing a client\n",
    "\n",
    "Prior to loading data, we need to prepare the client with specifications regarding the desired cohort and sampling rates. See section above for more information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(SUMMARY_PATH=SUMMARY_PATH)\n",
    "client.load_summary()\n",
    "\n",
    "client.prepare_cohorts(fold=0)\n",
    "client.set_sampling_rates(rates={\n",
    "    1: 0.5,\n",
    "    2: 0.5\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "At last, we are ready to use the client to load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    arrays = client.get()\n",
    "    print(arrays['dat'].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
