{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of Client\n",
    "\n",
    "The pipeline for loading data for machine learning training is a critical process that requires thoughtful and deliberate planning. Key considerations that need to be accounted for include:\n",
    "\n",
    "* keeping track of all data(s) and/or label(s) for each individual training case\n",
    "* consistent stratified sampling between training and validation splits\n",
    "* consistent stratified sampling between different data cohorts (e.g. from different hospitals or sources)\n",
    "* randomization of data loading order between epochs\n",
    "* any \"real time\" data preprocessing (e.g. normalization)\n",
    "\n",
    "Other advanced functionality may include:\n",
    "\n",
    "* in-memory loading of all data before training starts (if dataset is small)\n",
    "* asynchronous loading of data (if dataset is large)\n",
    "\n",
    "In this tutorial, we will cover the basics of creating a `client` for loading data, covering much of the key functionality described above. For each individual project, you must create your **own individualized** client for loading data. To help you get started, a template fully functional `client` is availabe in this repository at `/dl_core/clients/client.py`. Example usage of this module is available at the end of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts\n",
    "\n",
    "All terms in **bold** will reference specific concepts that are reused throughout this tutorial. Please review these terms before proceeding further.\n",
    "\n",
    "Machine learning algorithms require data to be **split** into *training* and *validation* sets. All training paradigms *require* this baseline division of data. For the purposes of this tutorial, a **split** represents the current usage of any particular example as *training* or *validation* data. Note that in the setting of cross-validation, all data will be used as *both* training and validation cases at different points during algorithm development.\n",
    "\n",
    "For certain problems, it is necessary to further subdivide data into **cohorts**. For the purposes of this tutorial, a **cohort** represents *any arbitrary* division of data into user defined subgroups. Why are **cohorts** important? It turns out that stratified sampling (e.g. selecting data at fixed rates from specific cohorts) oftentimes improves training dynamics for heterogenous datasets, including those commonly seen in medical problems. For example, given the low prevalence of most disease states, it is often beneficial to load data at an even 50-50% distribution between positive and negative examples (e.g supersample from the positive category). \n",
    "\n",
    "Given the above, two different sampling **rates** are defined:\n",
    "\n",
    "* **training_rate**: represents the rate of randomly selecting training / validation cases\n",
    "* **sampling_rate**: represents the rate at which each individual **cohort** is sampled from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Data Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the need to careful subselect splits or cohorts of data, it is often valuable to first extract key characteristics of data *first* as an independent step prior to loading any data. In this tutorial, implementation will including the following steps:\n",
    "\n",
    "* find all data(s) and/or label(s) for each individual training case\n",
    "* extract summary information about each training case\n",
    "* aggregate all summarized data\n",
    "* determine training / validation splits\n",
    "* serialize summary as a Pickle file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Data\n",
    "\n",
    "This portion of the code will be most the variable depending on the directory hierachy for the data in your project. In general, the goal will be to create a list of dictionaries, with each key-value pair representing a single full file path name. For example:\n",
    "\n",
    "```\n",
    "d = {\n",
    "    'dat_0': /full/path/to/dat/0,\n",
    "    'lbl_0': /full/path/to/lbl/0,\n",
    "    'lbl_1': /full/path/to/lbl/1, ... }\n",
    "```\n",
    "\n",
    "Note that the keys you choose can be arbitrary, as long as you remember what is what!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example, the data is currently organized as follows:\n",
    "\n",
    "```\n",
    "/hdfs/[ID-...0]/dat.hdf5\n",
    "/hdfs/[ID-...0]/bet.hdf5\n",
    "/hdfs/[ID-...1]/dat.hdf5\n",
    "/hdfs/[ID-...1]/lbl.hdf5\n",
    "...\n",
    "```\n",
    "\n",
    "Assuming that your data is organized in a similar way, here is a simple method to generate such a list of dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "\n",
    "def find_data(query):\n",
    "    \"\"\"\n",
    "    Method to read all data and make summary dictionary \n",
    "\n",
    "    :params\n",
    "\n",
    "      (dict) query : {\n",
    "\n",
    "        'root': '/path/to/root/dir',\n",
    "        [key_00]: [query_00],\n",
    "        [key_01]: [query_01], ...\n",
    "\n",
    "      }\n",
    "      \n",
    "    \"\"\"\n",
    "    assert 'root' in query\n",
    "    assert len(query) > 1\n",
    "\n",
    "    root = query.pop('root')\n",
    "    keys = sorted(query.keys())\n",
    "\n",
    "    q = query.pop(keys[0])\n",
    "    matches = glob.glob('%s/**/%s' % (root, q), recursive=True)\n",
    "\n",
    "    DATA = []\n",
    "    \n",
    "    for n, m in enumerate(matches):\n",
    "\n",
    "        print('CREATING SUMMARY (%07i/%07i): %s' % (n + 1, len(matches), m))\n",
    "\n",
    "        d = {keys[0]: m}\n",
    "        b = os.path.dirname(m)\n",
    "\n",
    "        # --- Find other matches\n",
    "        for key in keys[1:]:\n",
    "            ms = glob.glob('%s/%s' % (b, query[key]))\n",
    "\n",
    "            if len(ms) == 1: \n",
    "                d[key] = ms[0]\n",
    "                \n",
    "        if len(d) == len(keys):\n",
    "            DATA.append(d)\n",
    "    \n",
    "    return DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code demonstrates example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set query\n",
    "query = {\n",
    "    'root': '../../data/hdfs',\n",
    "    'dat': 'dat.hdf5',\n",
    "    'bet': 'bet.hdf5'}\n",
    "\n",
    "# --- Find data\n",
    "data = find_data(query)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Slice Location\n",
    "\n",
    "Next, we need to identify information about **each slice** of data which will be used for algorithm training. To do so, we will first create a system to reference each individual slice data using an **index** and a **coord** variable:\n",
    "\n",
    "* **index**: a value from `[0, n - 1]` representing the n-th sample in the dataset \n",
    "* **coord**: a *normalized* coordinate `[0, 1]` that represents the z-position of the slice\n",
    "\n",
    "After all the data has been loaded and summarized, we should have two vectors, `index` and `coord`, *equal in size* to the total number of slices of all data. For example, if we had five volumes, each with 10 slices, then:\n",
    "\n",
    "```\n",
    "index = [0, 0, 0 ..., 1, 1, 1 ..., 9, 9, 9, ... 9, 9, 9]\n",
    "coord = [0, 1, 2 ..., 0, 1, 2 ..., 0, 1, 2, ... 7, 8, 9] / 9\n",
    "```\n",
    "\n",
    "Assuming `data` contains a 4D Numpy volume, the following snippet of pseudocode will accomplish this:\n",
    "\n",
    "```\n",
    "index = []\n",
    "coord = []\n",
    "\n",
    "for INDEX in range(len(EXAMS)):\n",
    "\n",
    "    [... load data ...]\n",
    "    \n",
    "    index.append(np.ones(data.shape[0], dtype='int') * INDEX)\n",
    "    coord.append(np.arange(data.shape[0]) / (data.shape[0] - 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See below for a rough implementation of the above concepts. Keep in mind we need an additional variable, `LABELED`, which references the *key* in query from which to load data and use for calculations (in our case, `bet`). To load `*.hdf5` files we will use the `dl_core.io.hdf5` library. See dedicated notebook for more information.\n",
    "\n",
    "Both `index` and `coord` vectors are embedded in a return dictionary `META`. For all subsequent code, we will use this convention and simply continue adding more vectors of information to the same dictionary variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "PATH = '../../'\n",
    "if PATH not in sys.path:\n",
    "    sys.path.append('../../')\n",
    "from dl_core.io import hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_summary(query, LABELED):\n",
    "    \"\"\"\n",
    "    Method to read all data and make summary dictionary \n",
    "\n",
    "    :params\n",
    "\n",
    "      (dict) query : {\n",
    "\n",
    "        'root': '/path/to/root/dir',\n",
    "        [key_00]: [query_00],\n",
    "        [key_01]: [query_01], ...\n",
    "\n",
    "      }\n",
    "\n",
    "    \"\"\"\n",
    "    assert 'root' in query\n",
    "    assert len(query) > 1\n",
    "    assert LABELED in query\n",
    "\n",
    "    root = query.pop('root')\n",
    "    keys = sorted(query.keys())\n",
    "\n",
    "    q = query.pop(keys[0])\n",
    "    matches = glob.glob('%s/**/%s' % (root, q), recursive=True)\n",
    "\n",
    "    DATA = []\n",
    "    META = {}\n",
    "    META['index'] = []\n",
    "    META['coord'] = []\n",
    "\n",
    "    for n, m in enumerate(matches):\n",
    "\n",
    "        d = {keys[0]: m}\n",
    "        b = os.path.dirname(m)\n",
    "\n",
    "        # --- Find other matches\n",
    "        for key in keys[1:]:\n",
    "            ms = glob.glob('%s/%s' % (b, query[key]))\n",
    "\n",
    "            if len(ms) == 1: \n",
    "                d[key] = ms[0]\n",
    "\n",
    "        # --- Caculate summary meta information from LABELED\n",
    "        if len(d) == len(keys):\n",
    "\n",
    "            data, _ = hdf5.load(d[LABELED])\n",
    "\n",
    "            # --- Aggregate information\n",
    "            META['index'].append(np.ones(data.shape[0], dtype='int') * len(DATA))\n",
    "            META['coord'].append(np.arange(data.shape[0]) / (data.shape[0] - 1))\n",
    "            DATA.append(d)\n",
    "\n",
    "    # --- Concatenate all vectors\n",
    "    META = {k: np.concatenate(v) for k, v in META.items()}\n",
    "    \n",
    "    return DATA, META"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code demonstrates example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set query\n",
    "query = {\n",
    "    'root': '../../data/hdfs',\n",
    "    'dat': 'dat.hdf5',\n",
    "    'bet': 'bet.hdf5'}\n",
    "\n",
    "# --- Find data\n",
    "data, meta = make_summary(query, LABELED='bet')\n",
    "print(data)\n",
    "print(meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Slice Data\n",
    "\n",
    "In addition to `index` and `coord`, we may want to extract some additional information that may be useful when stratifying data loading in future. Oftentimes, this information will be based on the label values that are present at each slice, as this type of information will allow us to strategically load data at varying rates based on based on presence or absence of disease entity. \n",
    "\n",
    "In our example, the labels contain brain masks (1 == background, 2 == brain). Hypothetically, we may choose to load data such that 50% of slices contain backround, and 50% of slices contain brain. Thus, we will choose to now create two additional vectors, equal in size to the total number slices of all data (same as `index` and `coord`), that contain a binary True or False as to whether that slice contains background and/or brain. Both vectors will be added to the `META` return variable.\n",
    "\n",
    "The following snippet of pseudocode will accomplish this for an arbitrary number of `CLASSES`:\n",
    "\n",
    "```\n",
    "META = {}\n",
    "for INDEX in range(len(EXAMS)):\n",
    "\n",
    "    [... load data ...]\n",
    "    \n",
    "    for c in range(CLASSES + 1):\n",
    "        s = np.sum(data == c, axis=(1, 2, 3)) > 0\n",
    "        META[c].append(s)\n",
    "```\n",
    "\n",
    "**IMPORTANT NOTE**: In your specific project, you may want to stratify data loading in different ways (e.g. from different hospitals, etc). Whichever method you choose to divide up your cohorts, you **must** extract the relevant information at this step. In subsequent steps, we will use this information is some way to formally divide up cohorts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See below for a rough implementation of the above concepts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "PATH = '../../'\n",
    "if PATH not in sys.path:\n",
    "    sys.path.append('../../')\n",
    "from dl_core.io import hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_summary(query, LABELED, CLASSES=2):\n",
    "    \"\"\"\n",
    "    Method to read all data and make summary dictionary \n",
    "\n",
    "    :params\n",
    "\n",
    "      (dict) query : {\n",
    "\n",
    "        'root': '/path/to/root/dir',\n",
    "        [key_00]: [query_00],\n",
    "        [key_01]: [query_01], ...\n",
    "\n",
    "      }\n",
    "\n",
    "    \"\"\"\n",
    "    assert 'root' in query\n",
    "    assert len(query) > 1\n",
    "    assert LABELED in query\n",
    "\n",
    "    root = query.pop('root')\n",
    "    keys = sorted(query.keys())\n",
    "\n",
    "    q = query.pop(keys[0])\n",
    "    matches = glob.glob('%s/**/%s' % (root, q), recursive=True)\n",
    "\n",
    "    DATA = []\n",
    "    META = {c: [] for c in range(CLASSES + 1)}\n",
    "    META['index'] = []\n",
    "    META['coord'] = []\n",
    "\n",
    "    for n, m in enumerate(matches):\n",
    "\n",
    "        d = {keys[0]: m}\n",
    "        b = os.path.dirname(m)\n",
    "\n",
    "        # --- Find other matches\n",
    "        for key in keys[1:]:\n",
    "            ms = glob.glob('%s/%s' % (b, query[key]))\n",
    "\n",
    "            if len(ms) == 1: \n",
    "                d[key] = ms[0]\n",
    "\n",
    "        # --- Caculate summary meta information from LABELED\n",
    "        if len(d) == len(keys):\n",
    "\n",
    "            # --- Aggregate slice-by-slice label information\n",
    "            data, _ = hdf5.load(d[LABELED])\n",
    "\n",
    "            for c in range(CLASSES + 1):\n",
    "                s = np.sum(data == c, axis=(1, 2, 3)) > 0\n",
    "                META[c].append(s)\n",
    "\n",
    "            # --- Aggregate information\n",
    "            META['index'].append(np.ones(data.shape[0], dtype='int') * len(DATA))\n",
    "            META['coord'].append(np.arange(data.shape[0]) / (data.shape[0] - 1))\n",
    "            DATA.append(d)\n",
    "\n",
    "    # --- Concatenate all vectors\n",
    "    META = {k: np.concatenate(v) for k, v in META.items()}\n",
    "    \n",
    "    return DATA, META"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code demonstrates example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set query\n",
    "query = {\n",
    "    'root': '../../data/hdfs',\n",
    "    'dat': 'dat.hdf5',\n",
    "    'bet': 'bet.hdf5'}\n",
    "\n",
    "# --- Find data\n",
    "data, meta = make_summary(query, LABELED='bet', CLASSES=2)\n",
    "print(data)\n",
    "print(meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Train / Valid Splits\n",
    "\n",
    "Before we finish creating the required summary information, we must divide all the data up into training / validation splits. The easiest strategy here it to randomly assign all cases an integer between `[0, n - 1]` where `n` represents the total number of splits desired. During training, all cases that match a given **fold** will be set to validation data, while the remaining cases will be set to training data.\n",
    "\n",
    "As a simple example, to create an 80%-20% training/validation split, we set *n = 5*, e.g. all cases are randomly assigned a number from `[0, 4]`. In the first round of training, all cases assigned to `0` will be used for validation (~20%) while the remaining cases (~80%) will be used for training. From this, cross-validation is easy to implement by simply changing the validation **fold** from 0 to 4 which in turn updates the training/validation cases at each step. \n",
    "\n",
    "The following snippet of pseudocode will accomplish this for an arbitrary number of `N_FOLDS`:\n",
    "\n",
    "```\n",
    "valid = np.arange(N) % N_FOLDS\n",
    "valid = valid[np.random.permutation(valid.size)]\n",
    "```\n",
    "\n",
    "See below for a rough implementation of the above concepts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "PATH = '../../'\n",
    "if PATH not in sys.path:\n",
    "    sys.path.append('../../')\n",
    "from dl_core.io import hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_summary(query, LABELED, CLASSES=2, N_FOLDS=5):\n",
    "    \"\"\"\n",
    "    Method to read all data and make summary dictionary \n",
    "\n",
    "    :params\n",
    "\n",
    "      (dict) query : {\n",
    "\n",
    "        'root': '/path/to/root/dir',\n",
    "        [key_00]: [query_00],\n",
    "        [key_01]: [query_01], ...\n",
    "\n",
    "      }\n",
    "\n",
    "    \"\"\"\n",
    "    assert 'root' in query\n",
    "    assert len(query) > 1\n",
    "    assert LABELED in query\n",
    "\n",
    "    root = query.pop('root')\n",
    "    keys = sorted(query.keys())\n",
    "\n",
    "    q = query.pop(keys[0])\n",
    "    matches = glob.glob('%s/**/%s' % (root, q), recursive=True)\n",
    "\n",
    "    DATA = []\n",
    "    META = {c: [] for c in range(CLASSES + 1)}\n",
    "    META['index'] = []\n",
    "    META['coord'] = []\n",
    "\n",
    "    for n, m in enumerate(matches):\n",
    "\n",
    "        d = {keys[0]: m}\n",
    "        b = os.path.dirname(m)\n",
    "\n",
    "        # --- Find other matches\n",
    "        for key in keys[1:]:\n",
    "            ms = glob.glob('%s/%s' % (b, query[key]))\n",
    "\n",
    "            if len(ms) == 1: \n",
    "                d[key] = ms[0]\n",
    "\n",
    "        # --- Caculate summary meta information from LABELED\n",
    "        if len(d) == len(keys):\n",
    "\n",
    "            # --- Aggregate slice-by-slice label information\n",
    "            data, _ = hdf5.load(d[LABELED])\n",
    "\n",
    "            for c in range(CLASSES + 1):\n",
    "                s = np.sum(data == c, axis=(1, 2, 3)) > 0\n",
    "                META[c].append(s)\n",
    "\n",
    "            # --- Aggregate information\n",
    "            META['index'].append(np.ones(data.shape[0], dtype='int') * len(DATA))\n",
    "            META['coord'].append(np.arange(data.shape[0]) / (data.shape[0] - 1))\n",
    "            DATA.append(d)\n",
    "\n",
    "    # --- Set validation fold (N-folds)\n",
    "    valid = np.arange(len(META['index'])) % N_FOLDS\n",
    "    valid = valid[np.random.permutation(valid.size)]\n",
    "    META['valid'] = [np.ones(c.size) * v for c, v in zip(META['coord'], valid)]\n",
    "\n",
    "    # --- Concatenate all vectors\n",
    "    META = {k: np.concatenate(v) for k, v in META.items()}\n",
    "    \n",
    "    return DATA, META"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code demonstrates example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set query\n",
    "query = {\n",
    "    'root': '../../data/hdfs',\n",
    "    'dat': 'dat.hdf5',\n",
    "    'bet': 'bet.hdf5'}\n",
    "\n",
    "# --- Find data\n",
    "data, meta = make_summary(query, LABELED='bet', CLASSES=2)\n",
    "print(data)\n",
    "print(meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the variable `meta` contains a dictionary with multiple N-element vectors where N represents the total number of slices in the entire dataset.\n",
    "\n",
    "```\n",
    "meta['index'] = [0, 0, 0 ..., 1, 1, 1 ..., 9, 9, 9, ... 9, 9, 9]\n",
    "meta['coord'] = [0, 1, 2 ..., 0, 1, 2 ..., 0, 1, 2, ... 7, 8, 9] / 9\n",
    "meta[0] = [... presence or absence of 0 at this slice ...]\n",
    "meta[1] = [... presence or absence of 1 at this slice ...]\n",
    "meta[2] = [... presence or absence of 2 at this slice ...]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Cohorts\n",
    "\n",
    "Now that we have information extracted at all slice locations, we are ready to separate our data in cohorts. The cohorts will be split based on train / valid status as well as any potential cohort-specific requirements we may need for any particular project. In this example, we separated out slices into those containing: (1) background; (2) brain. Thus we will have a total of four separate cohorts:\n",
    "\n",
    "* train: background; brain\n",
    "* valid: background; brain\n",
    "\n",
    "Assuming that we have a series of N-element vectors, the easiest way to define these four cohorts is to simply create four sets (or lists) of indices, each set of which indicates the indices that are present in each cohort. Note that it is not necessary for the sets to be mutually exclusive.\n",
    "\n",
    "**IMPORTANT NOTE**: In your specific project, you may want to stratify data loading in different ways (e.g. from different hospitals, etc). Based on the information you extracted from data above, you **must** use that information now to properly create cohorts at this step. \n",
    "\n",
    "See below for a rough implementation of the above concepts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_cohorts(data, meta, fold):\n",
    "    \n",
    "    cohorts = {'train': {}, 'valid': {}}\n",
    "    \n",
    "    for split in ['train', 'valid']:\n",
    "\n",
    "        # --- Determine mask corresponding to current split \n",
    "        if split == 'train': \n",
    "            mask = meta['valid'] != fold\n",
    "        if split == 'valid':\n",
    "            mask = meta['valid'] == fold\n",
    "\n",
    "        # --- Find slices with class == 2\n",
    "        cohorts[split][2] = np.nonzero(meta[2] & mask)[0]\n",
    "\n",
    "        # --- Find slices with class == 1 (and not class == 2)\n",
    "        cohorts[split][1] = np.nonzero(meta[1] & ~meta[2] & mask)[0]\n",
    "    \n",
    "    return cohorts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code demonstrates example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohorts = prepare_cohorts(data, meta, fold=0)\n",
    "print(cohorts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Rates\n",
    "\n",
    "Now that the data is split into cohorts, we could easily control the rate at which sample from each group. We will keep track of the desired ratio using a simple dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_rates = {'train': 0.8, 'valid': 0.2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Rates\n",
    "\n",
    "In addition to rate at which we select training and validation cases, we would like to specify the rate at which we sample from each of our defined cohorts. In our example, we have two cohorts (`1` and `2`) which represent slices that contain background and brain, respectively. Although in our example we have a simple two-class division, we may in theory have multiple classes. To make the sampling problem easier in the future, we will parameterize this distribution by keeping track of the lower / upper bounds that correspond to each desired class.\n",
    "\n",
    "For example, if we had three classes (A, B and C) and desired a sampling distribution of 0.1, 0.4 and 0.5 respectively, we would have the following parameterization:\n",
    "\n",
    "```\n",
    "(class)  (lower)  (upper)  \n",
    "A        0.0      0.1\n",
    "B        0.1      0.5\n",
    "C        0.5      1.0\n",
    "```\n",
    "Given a random seed between `[0, 1]` it is now easy to \"pick\" one of these three classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_sampling_rates(cohorts, rates={}):\n",
    "\n",
    "    assert set(cohorts['train'].keys()) == set(rates.keys())\n",
    "    assert sum(list(rates.values())) == 1\n",
    "\n",
    "    keys = sorted(rates.keys())\n",
    "    vals = [rates[k] for k in keys]\n",
    "\n",
    "    lower = np.array([0] + vals[:-1])\n",
    "    upper = np.array(vals[1:] + [1])\n",
    "\n",
    "    return {\n",
    "        'cohorts': keys,\n",
    "        'lower': np.array(lower),\n",
    "        'upper': np.array(upper)} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code demonstrates example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rates = set_sampling_rates(cohorts, rates={1: 0.5, 2: 0.5}) \n",
    "print(sampling_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomizing Data\n",
    "\n",
    "Next, we need a mechanism to keep track of randomly selecting data from each cohort. To do so, for each cohort of size N, we create a corresponding randomly permuated vector of indices indicating the order from which to sample from each cohort. In our example, for 4 total cohorts, we need a total of 4 separate vectors of random numbers. \n",
    "\n",
    "At this time we will create an additional dictionary variable `current` that will keep track of the exact current index for each individual cohort, as well as count to total number of epochs per cohort.\n",
    "\n",
    "Let's see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_next_epoch(cohorts, indices, current, split, cohort):\n",
    "    \n",
    "    assert cohort in cohorts[split]\n",
    "    \n",
    "    indices[split][cohort] = np.random.permutation(cohorts[split][cohort].size)\n",
    "    current[split][cohort]['epoch'] += 1\n",
    "    current[split][cohort]['count'] = 0 \n",
    "    \n",
    "    return indices, current"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before any training begins, we initialize empty values for `indices` and `current`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = {'train': {}, 'valid': {}}\n",
    "current = {'train': {}, 'valid': {}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we loop through each of our defined cohorts and create randomized indices: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in ['train', 'valid']:\n",
    "    for cohort in cohorts[split]:\n",
    "        current[split][cohort] = {'epoch': -1, 'count': 0}\n",
    "        indices, current = prepare_next_epoch(cohorts, indices, current, split, cohort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Togther\n",
    "\n",
    "Let us now quickly review all the variables that we have prepared:\n",
    "\n",
    "* `cohorts`\n",
    "* `indices`\n",
    "* `current`\n",
    "* `training_rates`\n",
    "* `sampling_rates`\n",
    "\n",
    "Now let's see how we use all this information to load data from any given specified `split` and/or `cohort`, or to choose randomly if not defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_next_array(split=None, cohort=None):\n",
    "\n",
    "    if split is None:\n",
    "        split = 'train' if np.random.rand() < training_rates['train'] else 'valid'\n",
    "\n",
    "    if cohort is None:\n",
    "        if sampling_rates is not None:\n",
    "            i = np.random.rand()\n",
    "            i = (i < sampling_rates['upper']) & (i >= sampling_rates['lower'])\n",
    "            i = int(np.nonzero(i)[0])\n",
    "            cohort = sampling_rates['cohorts'][i]\n",
    "        else:\n",
    "            cohort = sorted(cohorts[split].keys())[0]\n",
    "\n",
    "    c = current[split][cohort]\n",
    "    i = indices[split][cohort]\n",
    "\n",
    "    if c['count'] > i.size - 1:\n",
    "        repare_next_epoch(split, cohort)\n",
    "        c = current[split][cohort]\n",
    "        i = indices[split][cohort]\n",
    "\n",
    "    index = meta['index'][i[c['count']]]\n",
    "    coord = meta['coord'][i[c['count']]]\n",
    "    data_ = data[index]\n",
    "\n",
    "    # --- Increment counter\n",
    "    c['count'] += 1\n",
    "\n",
    "    return data_, {'coord': coord, 'split': split, 'cohort': cohort}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code demonstrates example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    data_, meta_ = prepare_next_array()\n",
    "    print(data_)\n",
    "    print(meta_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading HDF5 Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "PATH = '../../'\n",
    "if PATH not in sys.path:\n",
    "    sys.path.append('../../')\n",
    "from dl_core.io import hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hdf5(fname, **kwargs):\n",
    "\n",
    "    infos = kwargs['infos'] if 'infos' in kwargs else None\n",
    "\n",
    "    return hdf5.load(fname, infos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper for Generic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(data, **kwargs):\n",
    "\n",
    "    if type(data) is not str:\n",
    "        return data\n",
    "\n",
    "    LOAD_FUNC = {\n",
    "        'hdf5': load_hdf5}\n",
    "\n",
    "    ext = data.split('.')[-1]\n",
    "\n",
    "    if ext in LOAD_FUNC:\n",
    "        return LOAD_FUNC[ext](data, **kwargs)\n",
    "\n",
    "    else:\n",
    "        print('ERROR provided extension is not supported: %s' % ext)\n",
    "        return None, {} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper for Dictionary of Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(data, **kwargs):\n",
    "\n",
    "    assert type(data) is dict\n",
    "\n",
    "    arrays = {}\n",
    "    for key, val in data.items():\n",
    "        arrays[key], _ = load(data=val, **kwargs)\n",
    "\n",
    "    return arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage of Template Client\n",
    "\n",
    "In this section, we will explore example usage of the template `client` provided in this repoository. \n",
    "\n",
    "**IMPORTANT**: This `client` has been written to load 1 x 512 x 512 (e.g. single slice) arrays from the provided example head CT. You *will need to modify* this code for your own individual projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dl_core\n",
    "\n",
    "To use the `dl_core` library, you need to ensure that the repository path has been set. If you are using the python interpreter directlying (e.g. command line) you will need to add the repository path to the `$PYTHONPATH` environment variable. If you are using an iPython interface (e.g. including Jupyter) you will need to set the path using the `sys` module: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set PATH to dl_core library path\n",
    "PATH = '../../' \n",
    "import sys\n",
    "if PATH not in sys.path:\n",
    "    sys.path.append(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_core.clients import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set default path locations\n",
    "SUMMARY_PATH = '../../data/pkls/summary.pkl'\n",
    "HDFS_PATH = '../../data/hdfs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a summary of the data\n",
    "\n",
    "Recall that in order to properly handle stratified sampling requirements, we need to know more information about the underlying data (e.g. which slice(s) are positive, etc). This information will then be used to randomize and organize cohorts for future data loading pipelines. See section above for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(SUMMARY_PATH=SUMMARY_PATH)\n",
    "\n",
    "client.make_summary(\n",
    "    query={\n",
    "        'root': HDFS_PATH,\n",
    "        'dat': 'dat.hdf5',\n",
    "        'bet': 'bet.hdf5'},\n",
    "    LABELED='bet',\n",
    "    CLASSES=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing a client\n",
    "\n",
    "Prior to loading data, we need to prepare the client with specifications regarding the desired cohort and sampling rates. See section above for more information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(SUMMARY_PATH=SUMMARY_PATH)\n",
    "client.load_summary()\n",
    "\n",
    "client.prepare_cohorts(fold=0)\n",
    "client.set_sampling_rates(rates={\n",
    "    1: 0.5,\n",
    "    2: 0.5\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "At last, we are ready to use the client to load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    arrays = client.get()\n",
    "    print(arrays['dat'].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
