{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of Client\n",
    "\n",
    "The pipeline for loading data for machine learning training is a critical process that requires thoughtful and deliberate planning. Key considerations that need to be accounted for include:\n",
    "\n",
    "* keeping track of all data(s) and/or label(s) for each individual training case\n",
    "* consistent stratified sampling between training and validation splits\n",
    "* consistent stratified sampling between different data cohorts (e.g. from different hospitals or sources)\n",
    "* randomization of data loading order between epochs\n",
    "* any \"real time\" data preprocessing (e.g. normalization)\n",
    "\n",
    "Other advanced functionality may include:\n",
    "\n",
    "* in-memory loading of all data before training starts (if dataset is small)\n",
    "* asynchronous loading of data (if dataset is large)\n",
    "\n",
    "In this tutorial, we will cover the basics of creating a `client` for loading data, covering much of the key functionality described above. For each individual project, you must create your **own individualized** client for loading data. To help you get started, a template fully functional `client` is availabe in this repository at `/dl_core/clients/client.py`. Example usage of this module is available at the end of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts\n",
    "\n",
    "All terms in **bold** will reference specific concepts that are reused throughout this tutorial. Please review these terms before proceeding further.\n",
    "\n",
    "Machine learning algorithms require data to be **split** into *training* and *validation* sets. All training paradigms *require* this baseline division of data. For the purposes of this tutorial, a **split** represents the current usage of any particular example as *training* or *validation* data. Note that in the setting of cross-validation, all data will be used as *both* training and validation cases at different points during algorithm development.\n",
    "\n",
    "For certain problems, it is necessary to further subdivide data into **cohorts**. For the purposes of this tutorial, a **cohort** represents *any arbitrary* division of data into user defined subgroups. Why are **cohorts** important? It turns out that stratified sampling (e.g. selecting data at fixed rates from specific cohorts) oftentimes improves training dynamics for heterogenous datasets, including those commonly seen in medical problems. For example, given the low prevalence of most disease states, it is often beneficial to load data at an even 50-50% distribution between positive and negative examples (e.g supersample from the positive category). \n",
    "\n",
    "Given the above, two different sampling **rates** are defined:\n",
    "\n",
    "* **training_rate**: represents the rate of randomly selecting training / validation cases\n",
    "* **sampling_rate**: represents the rate at which each individual **cohort** is sampled from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Data Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the need to careful subselect splits or cohorts of data, it is often valuable to first extract key characteristics of data *first* as an independent step prior to loading any data. In this tutorial, implementation will including the following steps:\n",
    "\n",
    "* find all data(s) and/or label(s) for each individual training case\n",
    "* extract summary information about each training case\n",
    "* aggregate all summarized data\n",
    "* determine training / validation splits\n",
    "* serialize summary as a Pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage\n",
    "\n",
    "In this section, we will explore example usage of the template `client` provided in this repoository. \n",
    "\n",
    "**IMPORTANT**: This `client` has been written to load 1 x 512 x 512 (e.g. single slice) arrays from the provided example head CT. You *will need to modify* this code for your own individual projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dl_core\n",
    "\n",
    "To use the `dl_core` library, you need to ensure that the repository path has been set. If you are using the python interpreter directlying (e.g. command line) you will need to add the repository path to the `$PYTHONPATH` environment variable. If you are using an iPython interface (e.g. including Jupyter) you will need to set the path using the `sys` module: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set PATH to dl_core library path\n",
    "PATH = '../../' \n",
    "\n",
    "# --- Use sys module to set $PYTHONPATH\n",
    "import sys\n",
    "if PATH not in sys.path:\n",
    "    sys.path.append(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_core.clients import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set default path locations\n",
    "SUMMARY_PATH = '../../data/pkls/summary.pkl'\n",
    "HDFS_PATH = '../../data/hdfs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a summary of the data\n",
    "\n",
    "Recall that in order to properly handle stratified sampling requirements, we need to know more information about the underlying data (e.g. which slice(s) are positive, etc). This information will then be used to randomize and organize cohorts for future data loading pipelines. See section above for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(SUMMARY_PATH=SUMMARY_PATH)\n",
    "\n",
    "client.make_summary(\n",
    "    query={\n",
    "        'root': HDFS_PATH,\n",
    "        'dat': 'dat.hdf5',\n",
    "        'bet': 'bet.hdf5'},\n",
    "    LABELED='bet',\n",
    "    CLASSES=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing a client\n",
    "\n",
    "Prior to loading data, we need to prepare the client with specifications regarding the desired cohort and sampling rates. See section above for more information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(SUMMARY_PATH=SUMMARY_PATH)\n",
    "client.load_summary()\n",
    "\n",
    "client.prepare_cohorts(fold=0)\n",
    "client.set_sampling_rates(rates={\n",
    "    1: 0.5,\n",
    "    2: 0.5\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "At last, we are ready to use the client to load data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    arrays = client.get()\n",
    "    print(arrays['dat'].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
