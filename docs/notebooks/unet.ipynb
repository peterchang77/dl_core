{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Your Environment\n",
    "\n",
    "The following lines of code will set up your python/iPython shell with the appropriate requirements and environment variables needed to run this tutorial, as well as download and prepare any necessary data. Of note, all required dependencies and additional tutorials are available at: https://github.com/peterchang77/dl_core. This GitHub repository contains the `dl_core` module and will be used throughout the tutorial. If a copy of this repository is present already, pass the complete path to the repository root directory to the `DL_PATH` variable as described below. \n",
    "\n",
    "The following arguments may be passed to the function:\n",
    "\n",
    "* `DL_PATH`: complete path to the `dl_core` library (GitHub repo); if not present, this path represents location where the GitHub repository will be cloned\n",
    "* `DS_PATH`: complete path to the dataset used in this library; if not present, this path represents location where the data will be downloaded\n",
    "* `DS_NAME`: name of dataset to be downloaded\n",
    "* `CUDA_VISIBLE_DEVICES`: the GPU device to use for training if multiple are available on machine (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O setenv.py https://raw.githubusercontent.com/peterchang77/dl_core/master/setenv.py\n",
    "from setenv import prepare_environment\n",
    "prepare_environment(\n",
    "    DL_PATH='../../',\n",
    "    DS_PATH='/data/raw/brats',\n",
    "    DS_NAME='brats')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import \n",
    "\n",
    "The following modules will be used in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import Input, Model, layers, models, losses, optimizers\n",
    "from tensorflow import math\n",
    "\n",
    "from dl_core.io import hdf5\n",
    "from dl_core.clients import Client\n",
    "from dl_core.vis import imshow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The data you have downloaded above contains preprocessed images and labels in HDF5 format. This data can be loaded directly using the `h5py` Python library, or using a high-level API as part of the `dl_core.io.hdf5` module. Let us load an example image and label pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Find data\n",
    "dirs = sorted(glob.glob('%s/hdfs/*/' % os.environ['DS_PATH']))\n",
    "print('A total of %i patients in dataset' % len(dirs))\n",
    "\n",
    "# --- Load first example using hdf5 module\n",
    "dat = '%sdat.hdf5' % dirs[0]\n",
    "lbl = '%slbl.hdf5' % dirs[0]\n",
    "\n",
    "dat = hdf5.load(dat)[0]\n",
    "lbl = hdf5.load(lbl)[0]\n",
    "\n",
    "# --- Inspect\n",
    "print(type(dat))\n",
    "print(dat.shape)\n",
    "print(lbl.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information about the HDF5 file format and the custom `hdf5` module as part of the `dl_core` library, see the following tutorial links (remote/local)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Now, let us view the underlying pixel information directly in Jupyter. To do so we will leverage the `imshow()` method available in the `dl_core.vis` module. This useful function can be used to directly visualize any 2D slice of data (first argument), as well as overlay any mask if optionally provided (second argument). Example usage as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- View middle slice\n",
    "z = int(dat.shape[0] / 2)\n",
    "imshow(dat[z], lbl[z])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Client\n",
    "\n",
    "To efficiently load the data used in this tutorial, a special `Client` class has been prepared that handles many of the low-level tasks that need to be accounted for during algorithm training such as keeping track of training / validation splits and randomization between epochs. Importantly, customized functionality for medical imaging is also supported, including stratified sampling by disease entity and statistics for image normalization. For more information about the `Client` class and how to customize for your own dataset, see the following tutorial links (remote/local). \n",
    "\n",
    "For this tutorial, we leverage this class by first creating a custom preprocessing method for this experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataClient(Client):\n",
    "    \n",
    "    def preprocess(self, arrays, meta=None):\n",
    "        \"\"\"\n",
    "        Method to preprocess arrays\n",
    "        \n",
    "        :params\n",
    "        \n",
    "          (np.ndarray) arrays['dat']: input data\n",
    "          (np.ndarray) arrays['lbl']: input labels\n",
    "          (dict) meta : metadata information about current data\n",
    "        \n",
    "        \"\"\"\n",
    "        # --- Preprocess data\n",
    "        import pdb\n",
    "        pdb.set_trace()\n",
    "        if meta is not None:\n",
    "            mu = meta['mu']\n",
    "            sd = meta['sd']\n",
    "        else:\n",
    "            mu = arrays['dat'].mean(axis=(0, 1, 2))\n",
    "            sd = arrays['dat'].std(axis=(0, 1, 2))\n",
    "        \n",
    "        mu = mu.reshape(1, 1, 1, -1)\n",
    "        sd = sd.reshape(1, 1, 1, -1)\n",
    "        arrays['dat'] = (arrays['dat'] - mu) / sd\n",
    "        \n",
    "        # --- Preprocess labels\n",
    "        arrays['lbl'] = (arrays['lbl'] >= 1).astype('uint8')\n",
    "        \n",
    "        return arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will instantiate a new `client` object, set a stratified sampling rate and prepare the first valildation fold. In our experiment, we will choose to stratify our data sampling such that an even 50-50% of loaded examples contain a positive or negative finding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Instantiate the data client\n",
    "client = DataClient()\n",
    "\n",
    "# --- Create cohorts\n",
    "client.prepare_cohorts(fold=0, cohorts={\n",
    "    1: lambda x : ~(x[1] | x[2] | x[3] | x[4]),\n",
    "    2: lambda x : x[1] | x[2] | x[3] | x[4]})\n",
    "\n",
    "# --- Set 50/50% sampling rate\n",
    "client.set_sampling_rates({\n",
    "    1: 0.5,\n",
    "    2: 0.5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this client to load data, we can either use the `client.get()` method to return a single example via a dictionary of arrays, or instantiate a Python generator that can be used to iterate through an entire batch of data in a memory efficient way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define input shape\n",
    "SHAPE = [1] + list(dat.shape[1:3])\n",
    "\n",
    "# --- Load a single train/valid using client.get()\n",
    "arrays = client.get(shape=SHAPE, split='train')\n",
    "arrays = client.get(shape=SHAPE, split='valid')\n",
    "\n",
    "# --- Load a single batch via a generator\n",
    "gen_train = client.generator(shape=SHAPE, split='train', batch_size=16)\n",
    "gen_valid = client.generator(shape=SHAPE, split='valid', batch_size=16)\n",
    "\n",
    "batch = next(gen_train)\n",
    "batch = next(gen_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "In this exercise, we will create a custom variant of the standard contracting-expanding netowrk topology, popularly referred to as a U-Net architecture. We will define the algorithm completely here in the next several code cells using the functional API of Tensorflow/Keras. For a more general overview of basic Tensorflow/Keras usage, see the following tutorial links (remote/local). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Convolutional Block\n",
    "\n",
    "To help facilitate concise and readable code, we will create template Python lambda functions to succintly define convolutional blocks, defined as the following series of consecutive operations:\n",
    "\n",
    "* convolution (or convolutional-transpose)\n",
    "* batch normalization\n",
    "* activation function (ReLU, leaky ReLU, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define convolution parameters\n",
    "kwargs = {\n",
    "    'kernel_size': (1, 3, 3),\n",
    "    'padding': 'same',\n",
    "    'kernel_initializer': 'he_normal'}\n",
    "\n",
    "# --- Define block components\n",
    "conv = lambda x, filters, strides : layers.Conv3D(filters=filters, strides=strides, **kwargs)(x)\n",
    "tran = lambda x, filters, strides : layers.Conv3DTranspose(filters=filters, strides=strides, **kwargs)(x)\n",
    "norm = lambda x : layers.BatchNormalization()(x)\n",
    "relu = lambda x : layers.LeakyReLU()(x)\n",
    "\n",
    "# --- Define stride-1, stride-2 blocks\n",
    "conv1 = lambda filters, x : relu(norm(conv(x, filters, strides=1)))\n",
    "conv2 = lambda filters, x : relu(norm(conv(x, filters, strides=(1, 2, 2))))\n",
    "tran2 = lambda filters, x : relu(norm(tran(x, filters, strides=(1, 2, 2)))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to define the full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape):\n",
    "    \"\"\"\n",
    "    Method to create simple U-Net architecture\n",
    "\n",
    "    \"\"\"\n",
    "    # --- Define input\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # --- Define contracting layers\n",
    "    l1 = conv1(16, inputs)\n",
    "    l2 = conv1(32, conv2(32, l1))\n",
    "    l3 = conv1(48, conv2(48, l2))\n",
    "    l4 = conv1(64, conv2(64, l3))\n",
    "    l5 = conv1(80, conv2(80, l4))\n",
    "\n",
    "    # --- Define expanding layers\n",
    "    l6 = tran2(64, l5)\n",
    "    l7 = tran2(48, conv1(64, l6 + l4))\n",
    "    l8 = tran2(32, conv1(48, l7 + l3))\n",
    "    l9 = tran2(16, conv1(32, l8 + l2))\n",
    "\n",
    "    logits = layers.Conv3D(filters=2, **kwargs)(conv1(16, l1 + l9))\n",
    "\n",
    "    return Model(inputs=inputs, outputs=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create model and show summary\n",
    "model = create_model(arrays['dat'].shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Model\n",
    "\n",
    "Next, we must compile the model with the requisite objects that define training dynamics (e.g. how the algorithm with learn). This will include classes that encapsulate the model `optimizer`, `loss` and `metrics` for evaluating algorithm performance. For more information about how these strategies are determined and defined, see the following tutorial links (remote/local)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define optimizer\n",
    "optimizer = optimizers.Adam(learning_rate=2e-4)\n",
    "\n",
    "# --- Define loss\n",
    "loss = losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Metric\n",
    "\n",
    "In addition the standard evaluation metric of `accuracy` (% of pixels or voxels that are predicted correctly), we will also keep track of a common metric to evaluate spatial overlap of masks: the Dice score. To do so, we need to create a custom metric function. For more information about creating custom metrics (and losses) in Tensorflow/Keras, see the following tutorial links (remote/local)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_dice():\n",
    "\n",
    "    def dice(y_true, y_pred):\n",
    "\n",
    "        true = y_true == 1\n",
    "        pred = y_pred[..., 1] > y_pred[..., 0] \n",
    "\n",
    "        A = math.count_nonzero(true & pred) * 2\n",
    "        B = math.count_nonzero(true) + math.count_nonzero(pred)\n",
    "\n",
    "        return A / B\n",
    "\n",
    "    return dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile\n",
    "\n",
    "At last we are ready to compile the model. This is done simply with a call using the `model.compile()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimzer=optimizer,\n",
    "    loss=loss,\n",
    "    metrics=['accuracy', metric_dice()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "One of the primary advantages to the Tensorflow/Keras API is that by following the above \"recipe\" to customize, instantiate and compile a `model` object, several very easy-to-use interfaces are available for algorithm training. In this tutorial, we will use data prepared from Python generators (`gen_train` and `gen_valid` as above) to train the model using the `model.fit_generator()` method. Usage is shown as follows using a single line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train the model\n",
    "model.fit_generator(\n",
    "    generator=gen_train,\n",
    "    steps_per_epoch=500,\n",
    "    epochs=4,\n",
    "    validation_data=gen_valid,\n",
    "    validation_steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "How did we do? The validation performance metrics (accuracy, Dice score) give us a reasonable benchmark, but the most important thing to do at the end of the day is to visually check some examples for yourself. Let us pass some validation data manually into the model using the `model.predict()` method and see some results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load data and preproces\n",
    "arrays = client.get(shape=SHAPE, split='valid')\n",
    "\n",
    "# --- Run prediction\n",
    "pred = model.predict(np.expand_dims(arrays['dat'], axis=0))\n",
    "mask = np.argmax(pred[0], axis=-1)\n",
    "\n",
    "# --- Show prediction\n",
    "imshow(arrays['dat'][0, ..., 1], mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading a Model\n",
    "\n",
    "After a model has been successfully trained, it can be saved and/or loaded by simply using the `model.save()` and `models.load_model()` methods. Note that any custom losses and/or metrics will need to be provided via a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Serialize a model\n",
    "os.makedirs('./models', exist_ok=True)\n",
    "model.save('./models/unet.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load a serialized model\n",
    "model = models.load_model('./models/unet.hdf5', custom_objects={'dice': metric_dice()})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
